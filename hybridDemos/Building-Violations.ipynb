{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Develop a Scala Spark Model on Chicago Building Violations\n",
    "\n",
    "\n",
    "Combine the strengths of Data Science Experience Cloud and DSX Local to model `Building Violations` in Chicago.\n",
    "___________\n",
    "This notebook runs on Scala 2.11 and Spark 2.0\n",
    "\n",
    "This notebook can be used as a companion to another [tutorial on our blog](https://medium.com/@adammassachi/dsx-hybrid-mode-91b580450c5b).   \n",
    "\n",
    "* The data are <a href=https://data.cityofchicago.org/Buildings/Building-Violations/22u3-xenr target=\"_blank\" rel=\"noopener noreferrer\">Violations issued by the Chicago Department of Buildings</a>\n",
    " over the period from 2006 until present. The dataset contains instances of `violations`. Each violation is assocaiated with an `inspection` and an `inspection status`. \n",
    "\n",
    "\n",
    "* Using Spark Machine Learning, we're going to develop a model for the data from 2006-2016 which provides a score in the interval $[0,1]$ for how likely we believe an individual building is to `Pass` or `Fail` an inspection. \n",
    "\n",
    "Download the data and bring it into your DSX project. \n",
    "______________\n",
    "## Table of contents\n",
    "1. [Wrangle data](#wrangle)\n",
    "2. [Build a pipeline](#build)\n",
    "3. [Train model](#train)\n",
    "4. [Save model](#save)\n",
    "5. [Deploy model](#deploy)\n",
    "_________________\n",
    "\n",
    "<a id=\"wrangle\"></a>\n",
    "### 1. Wrangle data \n",
    "Next, we’ll read the data into a Spark DataFrame. This is straightforward on DSX. First, upload the data to your project. You can also access this data via API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Import top level\n",
    "import scala.sys.process._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.util._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.catalyst.expressions.DateFormatClass\n",
    "import com.ibm.ibmos2spark.bluemix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//get your credentials\n",
    "var bmos = new bluemix(sc, configurationname, credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 2:===============================================>           (4 + 1) / 5]root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- VIOLATION LAST MODIFIED DATE: string (nullable = true)\n",
      " |-- VIOLATION DATE: string (nullable = true)\n",
      " |-- VIOLATION CODE: string (nullable = true)\n",
      " |-- VIOLATION STATUS: string (nullable = true)\n",
      " |-- VIOLATION STATUS DATE: string (nullable = true)\n",
      " |-- VIOLATION DESCRIPTION: string (nullable = true)\n",
      " |-- VIOLATION LOCATION: string (nullable = true)\n",
      " |-- VIOLATION INSPECTOR COMMENTS: string (nullable = true)\n",
      " |-- VIOLATION ORDINANCE: string (nullable = true)\n",
      " |-- INSPECTOR ID: string (nullable = true)\n",
      " |-- INSPECTION NUMBER: string (nullable = true)\n",
      " |-- INSPECTION STATUS: string (nullable = true)\n",
      " |-- INSPECTION WAIVED: string (nullable = true)\n",
      " |-- INSPECTION CATEGORY: string (nullable = true)\n",
      " |-- DEPARTMENT BUREAU: string (nullable = true)\n",
      " |-- ADDRESS: string (nullable = true)\n",
      " |-- PROPERTY GROUP: string (nullable = true)\n",
      " |-- SSA: string (nullable = true)\n",
      " |-- LATITUDE: string (nullable = true)\n",
      " |-- LONGITUDE: string (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Get the data\n",
    "// Add your credentials when you Insert the DataFrame\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.\n",
    "    builder().\n",
    "    getOrCreate()\n",
    "val violations = spark.\n",
    "    read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").\n",
    "    option(\"header\", \"true\").\n",
    "    option(\"inferSchema\", \"true\").\n",
    "    option(\"dateFormat\", \"MM/dd/yyyy\").\n",
    "    load(bmos.url(\"hybridDemos\", \"Building_Violations.csv\"))\n",
    "violations.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re not going to build a model on all of the data. We’ll need to separate 2006–2016 from 2017. We’ll use a decade of data to train, and then we’ll test the performance of our model on the 2017 data. Notice that in the above Schema, `VIOLATION DATE` , is string type. This means we’ll need to do some wrangling before we can filter by the dates in an intuitive way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Create datetime column\n",
    "val dated = violations.withColumn(\"timeStamp\", to_date(unix_timestamp(\n",
    "  $\"VIOLATION DATE\", \"MM/dd/yyyy\"\n",
    ").cast(\"timestamp\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s make some more modifications. First, we’ll rename all of the columns so that we can reference them more easily later. \n",
    "We'll also remove the space between the names and replace with an underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// sub whitespace for `_`\n",
    "var cleanDf = dated\n",
    "for(col <- dated.columns){\n",
    "    cleanDf = cleanDf.withColumnRenamed(col,col.replaceAll(\"\\\\s\", \"_\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re modeling `INSPECTION_STATUS`, but there are a small number of records where the status has not been resolved into `PASSED` or `FAILED`. We can select only those records that meet our criteria with `SQL Transformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.SQLTransformer\n",
    "val df = new SQLTransformer().setStatement(\"SELECT * FROM __THIS__ WHERE INSPECTION_STATUS IN ('FAILED', 'PASSED')\").transform(cleanDf)\n",
    "val preppedFrame = df.withColumn(\"LATITUDE\", df(\"LATITUDE\").cast(DoubleType)).\n",
    "                    withColumn(\"LONGITUDE\", df(\"LONGITUDE\").cast(DoubleType))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also change the datatype of LATITUDE and LONGITUDE from string to Double. Now we’ll separate the data by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Filter by date. Train on  year < 2017, test on 2017 data\n",
    "val trainingData2016 = preppedFrame.filter(year($\"timestamp\").leq(lit(2016))) \n",
    "val testingData2017 = preppedFrame.filter(year($\"timestamp\").gt(lit(2016))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `leq` is `less-than-or-equal-to` . Then `gt` should be easy to guess.\n",
    "Now, we’ve represented the DataFrame with a new field, timeStamp . We can use this to filter the timestamp data intuitively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|VIOLATION_DATE|\n",
      "+--------------+\n",
      "|    08/15/2017|\n",
      "|    05/09/2017|\n",
      "|    02/28/2017|\n",
      "+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Take a peek\n",
    "testingData2017.select(\"VIOLATION_DATE\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|VIOLATION_DATE|\n",
      "+--------------+\n",
      "|    07/24/2007|\n",
      "|    04/01/2008|\n",
      "|    04/01/2008|\n",
      "+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// at the training data too\n",
    "trainingData2016.select(\"VIOLATION_DATE\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we’re going to choose only a subset of the fields to use for modeling. Many of the other fields have numerous missing values, which is slightly beyond the scope of this tutorial. \n",
    "First, we’ll specify a subset of the columns. Then we’ll drop those rows which contain nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val keepCols = Array(\"VIOLATION_CODE\", \"VIOLATION_DESCRIPTION\", \n",
    "                   \"INSPECTION_STATUS\", \"INSPECTOR_ID\", \n",
    "                   \"INSPECTION_CATEGORY\", \"DEPARTMENT_BUREAU\", \n",
    "                   \"LATITUDE\", \"LONGITUDE\")\n",
    "val dfTrain = trainingData2016.select(keepCols.head, keepCols.tail: _*).na.drop\n",
    "val dfTest = testingData2017.select(keepCols.head, keepCols.tail: _*).na.drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* save the testing df for later */\n",
    "val written = dfTest.coalesce(1).write.option(\"header\", \"true\").csv(\"buildingTest2017OK.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VIOLATION_CODE: string (nullable = true)\n",
      " |-- VIOLATION_DESCRIPTION: string (nullable = true)\n",
      " |-- INSPECTION_STATUS: string (nullable = true)\n",
      " |-- INSPECTOR_ID: string (nullable = true)\n",
      " |-- INSPECTION_CATEGORY: string (nullable = true)\n",
      " |-- DEPARTMENT_BUREAU: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTrain.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"build\"></a>\n",
    "### 2. Build Pipeline\n",
    "When deploying a model to Watson Machine Learning, we need to provide a `Spark Machine Learning Pipeline` which indicates how to transform raw data into the representation required by our model. Pipelines typically include a series of transformers and terminate with a model or, especially in classification tasks, some transformer which will convert model predictions into string labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/* Import transformers to build a pipeline [...] */ \n",
    "import org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorAssembler}\n",
    "import org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer}\n",
    "import org.apache.spark.ml.feature.{HashingTF, IDF}\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use the `StringIndexer` in order to convert strings into a numeric representation for the machine. You can read about many transformations in the documentation. We assign each transformation a value because we’ll need to reference them later in the Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 5:===============================================>           (4 + 1) / 5]"
     ]
    }
   ],
   "source": [
    "// Label colum\n",
    "val labelCol = new StringIndexer().setInputCol(\"INSPECTION_STATUS\").setOutputCol(\"STATUS_LABEL\").fit(df)\n",
    "\n",
    "// Feature cols with String Indexer => Vector Assembler //\n",
    "\n",
    "//* VIOLATION CODE * //\n",
    "val interCodeCol = new StringIndexer().setInputCol(\"VIOLATION_CODE\").setOutputCol(\"CODE_X\").setHandleInvalid(\"skip\")\n",
    "\n",
    "\n",
    "//* INSPECTOR ID * //\n",
    "val interSpector = new StringIndexer().setInputCol(\"INSPECTOR_ID\").setOutputCol(\"INSP_X\").setHandleInvalid(\"skip\")\n",
    "\n",
    "\n",
    "//* INSPECTION CATEGORY * //\n",
    "val interCatSpector = new StringIndexer().setInputCol(\"INSPECTION_CATEGORY\").setOutputCol(\"INCAT_X\").setHandleInvalid(\"skip\")\n",
    "\n",
    "\n",
    "//* DEPARTMENT BUREAU * //\n",
    "val interBureau = new StringIndexer().setInputCol(\"DEPARTMENT_BUREAU\").setHandleInvalid(\"skip\").setOutputCol(\"BUR_X\")\n",
    "\n",
    "\n",
    "//** DEALING WITH TEXT **//\n",
    "val regexTokenizer = new RegexTokenizer().setInputCol(\"VIOLATION_DESCRIPTION\").setOutputCol(\"WORD_X\").setPattern(\"\\\\W\")\n",
    "val hashingTF = new HashingTF().setInputCol(\"WORD_X\").setOutputCol(\"DESCRIPTION\").setNumFeatures(150) // experiment with numFeatures + regularization params\n",
    "\n",
    "// LAT AND LONG ARE NUMERIC //\n",
    "\n",
    "\n",
    "//** VECTOR ASSEMBLER **//\n",
    "\n",
    "val vecAssembler = new VectorAssembler().setInputCols(Array(\"BUR_X\", \"INCAT_X\", \"CODE_X\", \"INSP_X\", \"DESCRIPTION\", \"LATITUDE\", \"LONGITUDE\")).setOutputCol(\"FEATURES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that after creating a new instance of `StringIndexer` , we use `setInputCol` and `setOutputCol` . The output column will go into the `VectorAssembler`. All of those features we use for modeling we’ll include in VectorAssembler.\n",
    "But what about string data that is not categorical? Sure, we can index all of the `INSPECTOR_ID` data, but does that make sense for the `VIOLATION_DESCRIPTION` , where almost every field is unique? \n",
    "<br>\n",
    "\n",
    "For text data like this, Scala and Spark provide other handy transformations. For `RegexTokenizer` and `HashingTF` the general idea is simple. We’re going to take the text and break it into individual words, called `tokens`, with the tokenizer. Then we map the tokens contained in each violation description to their frequencies. This will allow us to accept unseen data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.{Model, Pipeline, PipelineStage, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "\n",
    "//** Logistic Regression **//\n",
    "val logitModel = new LogisticRegression().setLabelCol(\"STATUS_LABEL\").setFeaturesCol(\"FEATURES\").setRegParam(0.1)\n",
    "\n",
    "\n",
    "//** Convert index prediction back to string **//\n",
    "val labelConverter = new IndexToString().setInputCol(\"prediction\").setOutputCol(\"PREDICTED_LABEL\").setLabels(labelCol.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the modeling pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/* Logitic Regression Pipeline */ \n",
    "val logisticPipe = new Pipeline().setStages(\n",
    "                                    Array(\n",
    "                                        labelCol, \n",
    "                                        interCodeCol, \n",
    "                                        interSpector, \n",
    "                                        interCatSpector,\n",
    "                                        interBureau,\n",
    "                                        regexTokenizer, hashingTF,\n",
    "                                        vecAssembler,\n",
    "                                        logitModel                                                                  \n",
    "                                    )\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "### 3. Train models\n",
    "Just call `.fit()` on the pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 15:==============================================>           (4 + 1) / 5]"
     ]
    }
   ],
   "source": [
    "val trainedLogit = logisticPipe.fit(dfTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make predictions and get metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// predict\n",
    "val predictionsLogisitc = trainedLogit.transform(dfTest)\n",
    "\n",
    "\n",
    "// Prepare for metrics\n",
    "val predictionAndLabels = predictionsLogisitc.select(\"STATUS_LABEL\", \"prediction\").rdd.map(row => \n",
    "            (row.getAs[Double](\"prediction\"), row.getAs[Double](\"STATUS_LABEL\")))\n",
    "\n",
    "val metrics = new BinaryClassificationMetrics(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a new object metrics which contains a lot of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 159:=============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6834321232738194"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// AUC\n",
    "metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save\"></a>\n",
    "### 4. Save Model\n",
    "You’ll need an instance of `Watson Machine Learning` on Bluemix. You can create a new instance directly from within DSX, but you’ll need to log in to Bluemix for your credentials. Check the companion [blog](https://medium.com/p/91b580450c5b/edit) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import com.ibm.analytics.ngp.repository._\n",
    "\n",
    "// Helper libraries\n",
    "\n",
    "import scalaj.http.{Http, HttpOptions}\n",
    "import scala.util.{Success, Failure}\n",
    "import java.util.Base64\n",
    "import java.nio.charset.StandardCharsets\n",
    "import play.api.libs.json._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//@hidden_cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Success(())"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Authorize\n",
    "val client = MLRepositoryClient(service_path)\n",
    "client.authorize(username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// model artifact\n",
    "val model_artifact = MLRepositoryArtifact(trainedLogit, dfTrain, \"VIOLATIONS_SCALA211_SPARK20\")\n",
    "val saved = client.models.save(model_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Success(com.ibm.analytics.ngp.repository.MLRepositoryClient$ModelAdapter$$anon$1@edd63bce)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelType: Some(sparkml-model-2.0)\n",
      "trainingDataSchema: Some({\"type\":\"struct\",\"fields\":[{\"name\":\"VIOLATION_CODE\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"VIOLATION_DESCRIPTION\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"INSPECTION_STATUS\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"INSPECTOR_ID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"INSPECTION_CATEGORY\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DEPARTMENT_BUREAU\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"LATITUDE\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"LONGITUDE\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]})\n",
      "creationTime: Some(2017-10-05T17:48:26.849Z)\n",
      "modelVersionHref: Some(https://ibm-watson-ml.mybluemix.net/v2/artifacts/models/bcbf46f6-a5bc-4ed9-bc78-a90940711c6b/versions/887800a7-ffc3-4a82-885d-428fe63a2461)\n",
      "label: Some(INSPECTION_STATUS)\n",
      "runtime: Some(spark-2.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"modelType: \" + saved.get.meta.prop(\"modelType\"))\n",
    "println(\"trainingDataSchema: \" + saved.get.meta.prop(\"trainingDataSchema\"))\n",
    "println(\"creationTime: \" + saved.get.meta.prop(\"creationTime\"))\n",
    "println(\"modelVersionHref: \" + saved.get.meta.prop(\"modelVersionHref\"))\n",
    "println(\"label: \" + saved.get.meta.prop(\"label\"))\n",
    "println(\"runtime: \"+ saved.get.meta.prop(\"runtime\"))\n",
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deploy\"></a>\n",
    "### 5. Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're following along in the [blog](https://medium.com/p/91b580450c5b/edit), this is where we'll deploy through the UI. \n",
    "\n",
    "____________\n",
    "\n",
    "### Author\n",
    "Adam Massachi is a Data Science Intern with the Data Science Experience and Watson Data Platform teams at IBM. Before IBM, he worked on political campaigns, building and managing large volunteer operations and organizing campaign finance initiatives. Say hello [@adammassach](https://twitter.com/adammassach?lang=en)!\n",
    "\n",
    "### Citations\n",
    "\n",
    "City of Chicago (2017). Building Violations <a href=https://data.cityofchicago.org/Buildings/Building-Violations/22u3-xenr target=\"_blank\" rel=\"noopener noreferrer\">https://data.cityofchicago.org/Buildings/Building-Violations/22u3-xenr</a>  Chicago, IL: Chicago City Data Portal\n",
    "\n",
    "Copyright © IBM Corp. 2017. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11 with Spark 2.0",
   "language": "scala",
   "name": "scala-spark20"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
